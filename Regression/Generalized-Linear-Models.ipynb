{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#平方误差损失函数最小，通过普通最小二乘法求解系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "0.9999999999999996\n",
      "[ 7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "#sklearn包中线性回归模型，通过最小二乘法求解回归系数\n",
    "#损失函数为误差平方和\n",
    "#下面例子用来拟合y=x+1方程\n",
    "from sklearn import linear_model\n",
    "lm1 = linear_model.LinearRegression()\n",
    "#用训练集训练模型参数\n",
    "lm1.fit([[0],[1],[2],[3],[4]],[1,2,3,4,5])\n",
    "print(lm1.coef_)\n",
    "print(lm1.intercept_)\n",
    "print(lm1.predict([[6],[7],[8],[9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn中岭回归，由于最小二乘线性回归中的损失函数只有误差平方和项，即只有经验风险最小化项\n",
    "#没有结构风险最小化项，可能导致模型泛化性很差，岭回归误差函数中引入正则化项\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99009901]\n",
      "1.0198019801980198\n",
      "[0.90909091]\n",
      "1.1818181818181819\n",
      "[0.09090909]\n",
      "2.8181818181818183\n",
      "[0.00990099]\n",
      "2.98019801980198\n",
      "[6.96039604 7.95049505 8.94059406 9.93069307]\n",
      "[6.63636364 7.54545455 8.45454545 9.36363636]\n",
      "[3.36363636 3.45454545 3.54545455 3.63636364]\n",
      "[3.03960396 3.04950495 3.05940594 3.06930693]\n"
     ]
    }
   ],
   "source": [
    "#sklearn包中岭回归模型，\n",
    "#下面例子用来拟合y=x+1方程，岭回归中惩罚参数越大模型中的回归系数越小，下面分别以\n",
    "#0.1,1,100,1000来说明\n",
    "\n",
    "from sklearn import linear_model\n",
    "Rm1 = linear_model.Ridge(alpha=0.1)\n",
    "Rm2 = linear_model.Ridge(alpha=1)\n",
    "Rm3 = linear_model.Ridge(alpha=100)\n",
    "Rm4 = linear_model.Ridge(alpha=1000)\n",
    "\n",
    "Rm1.fit([[0],[1],[2],[3],[4]],[1,2,3,4,5])\n",
    "Rm2.fit([[0],[1],[2],[3],[4]],[1,2,3,4,5])\n",
    "Rm3.fit([[0],[1],[2],[3],[4]],[1,2,3,4,5])\n",
    "Rm4.fit([[0],[1],[2],[3],[4]],[1,2,3,4,5])\n",
    "\n",
    "print(Rm1.coef_)\n",
    "print(Rm1.intercept_)\n",
    "\n",
    "print(Rm2.coef_)\n",
    "print(Rm2.intercept_)\n",
    "\n",
    "print(Rm3.coef_)\n",
    "print(Rm3.intercept_)\n",
    "\n",
    "print(Rm4.coef_)\n",
    "print(Rm4.intercept_)\n",
    "\n",
    "print(Rm1.predict([[6],[7],[8],[9]]))\n",
    "print(Rm2.predict([[6],[7],[8],[9]]))\n",
    "print(Rm3.predict([[6],[7],[8],[9]]))\n",
    "print(Rm4.predict([[6],[7],[8],[9]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上可见随着alpha增大,coef_的值越来越小，下面以数据有噪声为例，比较岭回归和最小二乘线性回归的优缺点，待拟合的方程仍然为y=x+1,训练数据加入随机噪声\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16702484]\n",
      "0.8700535294882079\n",
      "[ 6.70517771 12.54030189 24.21055024 35.8807986 ]\n",
      "39.40998876678873\n",
      "[1.144142]\n",
      "0.9272606292779426\n",
      "[ 6.64797061 12.36868059 23.81010055 35.2515205 ]\n",
      "30.56118533791461\n",
      "[1.06093167]\n",
      "1.1352864466951575\n",
      "[ 6.43994479 11.74460313 22.35391982 32.96323651]\n",
      "30.56118533791461\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "#生产训练集合测试集\n",
    "noise = np.random.rand(4)\n",
    "X_train=[[1],[2],[3],[4]]\n",
    "Y_train=[2,3,4,5]+noise\n",
    "X_test=[[5],[10],[20],[30]]\n",
    "Y_test=[5,11,21,31]\n",
    "#训练模型\n",
    "lm1 = linear_model.LinearRegression()\n",
    "lm1.fit(X_train,Y_train)\n",
    "#预测，并计算预测值和测试值之间的误差平方和\n",
    "lm1_predict=lm1.predict(X_test)\n",
    "error = np.sum(np.square(lm1_predict-Y_test))\n",
    "#输出结果\n",
    "print(lm1.coef_)\n",
    "print(lm1.intercept_)\n",
    "print(lm1_predict)\n",
    "print(error)\n",
    "#训练岭回归模型，alpha=0.1\n",
    "rm1 = linear_model.Ridge(alpha=0.1)\n",
    "rm1.fit(X_train,Y_train)\n",
    "\n",
    "#预测，并计算预测值和测试值之间的误差平方和\n",
    "rm1_predict=rm1.predict(X_test)\n",
    "error = np.sum(np.square(rm1_predict-Y_test))\n",
    "\n",
    "#输出岭回归预测结果和误差\n",
    "print(rm1.coef_)\n",
    "print(rm1.intercept_)\n",
    "print(rm1_predict)\n",
    "print(error)\n",
    "\n",
    "#训练岭回归模型，alpha=0.5\n",
    "rm2 = linear_model.Ridge(alpha=0.5)\n",
    "rm2.fit(X_train,Y_train)\n",
    "\n",
    "#预测，并计算预测值和测试值之间的误差平方和\n",
    "rm2_predict=rm2.predict(X_test)\n",
    "error = np.sum(np.square(rm1_predict-Y_test))\n",
    "\n",
    "\n",
    "#输出岭回归模型预测结果和误差\n",
    "print(rm2.coef_)\n",
    "print(rm2.intercept_)\n",
    "print(rm2_predict)\n",
    "print(error)\n",
    "\n",
    "#通过交叉验证选择alpha的值\n",
    "alpha = linear_model.RidgeCV(alphas=[0.1,0.3,0.5,0.8,1.0])\n",
    "alpha.fit(X_train,Y_train)\n",
    "print(alpha.alpha_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上可以看出,噪声较小时，线性回归效果最好，加大数据噪声，noise = np.random.rand(4)，再次进行预测，发现预测结果如下：\n",
    "\n",
    "[1.16702484]\n",
    "0.8700535294882079\n",
    "[ 6.70517771 12.54030189 24.21055024 35.8807986 ]\n",
    "39.40998876678873\n",
    "[1.144142]\n",
    "0.9272606292779426\n",
    "[ 6.64797061 12.36868059 23.81010055 35.2515205 ]\n",
    "30.56118533791461\n",
    "[1.06093167]\n",
    "1.1352864466951575\n",
    "[ 6.43994479 11.74460313 22.35391982 32.96323651]\n",
    "30.56118533791461\n",
    "0.1\n",
    "alpha等于0.1时，岭回归预测效果最好\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
